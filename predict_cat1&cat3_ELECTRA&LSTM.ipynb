{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc72c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import TFElectraModel\n",
    "from transformers import ElectraTokenizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from konlpy.tag import Komoran\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858e4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2561c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6493fc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    train['overview'][i] = train['overview'][i].replace('\\n','').replace('\\t','').replace('\\r','').replace('<br>', '').replace('<br />','').replace('*','')\n",
    "    train['overview'][i] = train['overview'][i].strip()\n",
    "\n",
    "for i in range(len(test)):\n",
    "    test['overview'][i] = test['overview'][i].replace('\\n','').replace('\\t','').replace('\\r','').replace('<br>', '').replace('<br />','').replace('*','')\n",
    "    test['overview'][i] = test['overview'][i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e11bcd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['자연', '레포츠', '음식', '인문(문화/예술/역사)', '숙박', '쇼핑'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['cat1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c69c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat1_1 = len(train[train['cat1']=='자연'])\n",
    "cat1_2 = len(train[train['cat1']=='레포츠'])\n",
    "cat1_3 = len(train[train['cat1']=='음식'])\n",
    "cat1_4 = len(train[train['cat1']=='인문(문화/예술/역사)'])\n",
    "cat1_5 = len(train[train['cat1']=='숙박'])\n",
    "cat1_6 = len(train[train['cat1']=='쇼핑'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bf84a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat1=>자연: 1719\n",
      "cat1=>레포츠: 2611\n",
      "cat1=>음식: 4912\n",
      "cat1=>인문: 5614\n",
      "cat1=>숙박: 1434\n",
      "cat1=>쇼핑: 696\n",
      "전체:16986, cat1_1~6의 합: 16986\n"
     ]
    }
   ],
   "source": [
    "print(f\"cat1=>자연: {cat1_1}\")\n",
    "print(f\"cat1=>레포츠: {cat1_2}\")\n",
    "print(f\"cat1=>음식: {cat1_3}\")\n",
    "print(f\"cat1=>인문: {cat1_4}\")\n",
    "print(f\"cat1=>숙박: {cat1_5}\")\n",
    "print(f\"cat1=>쇼핑: {cat1_6}\")\n",
    "print(f\"전체:{len(train)}, cat1_1~6의 합: {cat1_1+cat1_2+cat1_3+cat1_4+cat1_5+cat1_6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "324bf26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = train[train['cat1']=='자연']\n",
    "train_2 = train[train['cat1']=='레포츠']\n",
    "train_3 = train[train['cat1']=='음식']\n",
    "train_4 = train[train['cat1']=='인문(문화/예술/역사)']\n",
    "train_5 = train[train['cat1']=='숙박']\n",
    "train_6 = train[train['cat1']=='쇼핑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9499ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['항구/포구' '섬' '자연휴양림' '해수욕장' '산' '희귀동.식물' '수목원' '강' '자연생태관광지' '계곡' '폭포'\n",
      " '국립공원' '해안절경' '기암괴석' '호수' '동굴' '도립공원' '군립공원' '약수터' '등대']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(train_1['cat3'].unique())\n",
    "print(len(train_1['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "857ab7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['골프' '야영장,오토캠핑장' '스키(보드) 렌탈샵' '복합 레포츠' '자동차경주' '자전거하이킹' '썰매장' '요트' '래프팅'\n",
      " '승마' '트래킹' '수련시설' '민물낚시' '바다낚시' '수영' '카약/카누' '카지노' '윈드서핑/제트스키' '번지점프'\n",
      " '카트' 'MTB' '스케이트' '스카이다이빙' '헹글라이딩/패러글라이딩' '스노쿨링/스킨스쿠버다이빙'\n",
      " '인라인(실내 인라인 포함)' '사격장' 'ATV' '빙벽등반' '수상레포츠' '스키/스노보드']\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(train_2['cat3'].unique())\n",
    "print(len(train_2['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0429064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한식' '일식' '바/까페' '채식전문점' '중식' '서양식' '패밀리레스토랑' '클럽']\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(train_3['cat3'].unique())\n",
    "print(len(train_3['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8339aedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['일반축제' '유적지/사적지' '전시관' '관광단지' '컨벤션' '성' '안보관광' '이색거리' '미술관/화랑' '공원' '박물관'\n",
      " '도서관' '공연장' '농.산.어촌 체험' '기념탑/기념비/전망대' '기념관' '유명건물' '유원지' '사찰' '박람회'\n",
      " '문화전수시설' '종교성지' '동상' '기타행사' '고택' '문화원' '온천/욕장/스파' '기타' '테마공원' '유람선/잠수함관광'\n",
      " '외국문화원' '다리/대교' '분수' '고궁' '민속마을' '전통공연' '문' '식음료' '터널' '문화관광축제' '발전소'\n",
      " '대중콘서트' '대형서점' '생가' '영화관' '이색찜질방' '학교' '헬스투어' '연극' '컨벤션센터' '뮤지컬' '이색체험'\n",
      " '클래식음악회']\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "print(train_4['cat3'].unique())\n",
    "print(len(train_4['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "753aab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['모텔' '한옥스테이' '펜션' '게스트하우스' '홈스테이' '콘도미니엄' '민박' '유스호스텔' '서비스드레지던스']\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(train_5['cat3'].unique())\n",
    "print(len(train_5['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "160e6af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['전문상가' '5일장' '상설시장' '공예,공방' '백화점' '면세점' '특산물판매점']\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "print(train_6['cat3'].unique())\n",
    "print(len(train_6['cat3'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a794bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\2919564192.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_1['cat3'] = train_1_encoder.transform(train_1['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_1_encoder = LabelEncoder()\n",
    "\n",
    "train_1_encoder.fit(train_1['cat3'])\n",
    "train_1['cat3'] = train_1_encoder.transform(train_1['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d45ff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\115441887.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_2['cat3'] = train_2_encoder.transform(train_2['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_2_encoder = LabelEncoder()\n",
    "\n",
    "train_2_encoder.fit(train_2['cat3'])\n",
    "train_2['cat3'] = train_2_encoder.transform(train_2['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "980b29ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\904434580.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_3['cat3'] = train_3_encoder.transform(train_3['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_3_encoder = LabelEncoder()\n",
    "\n",
    "train_3_encoder.fit(train_3['cat3'])\n",
    "train_3['cat3'] = train_3_encoder.transform(train_3['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7aeab395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\3496481789.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_4['cat3'] = train_4_encoder.transform(train_4['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_4_encoder = LabelEncoder()\n",
    "\n",
    "train_4_encoder.fit(train_4['cat3'])\n",
    "train_4['cat3'] = train_4_encoder.transform(train_4['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9b2e547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\423763710.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_5['cat3'] = train_5_encoder.transform(train_5['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_5_encoder = LabelEncoder()\n",
    "\n",
    "train_5_encoder.fit(train_5['cat3'])\n",
    "train_5['cat3'] = train_5_encoder.transform(train_5['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "279fd156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Local\\Temp\\ipykernel_2148\\2536908416.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_6['cat3'] = train_6_encoder.transform(train_6['cat3'])\n"
     ]
    }
   ],
   "source": [
    "train_6_encoder = LabelEncoder()\n",
    "\n",
    "train_6_encoder.fit(train_6['cat3'])\n",
    "train_6['cat3'] = train_6_encoder.transform(train_6['cat3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05b5b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff7a911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1232 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "train_encoded_electra_1 = tokenizer.batch_encode_plus(train_1['overview'].values.tolist())\n",
    "train_encoded_electra_2 = tokenizer.batch_encode_plus(train_2['overview'].values.tolist())\n",
    "train_encoded_electra_3 = tokenizer.batch_encode_plus(train_3['overview'].values.tolist())\n",
    "train_encoded_electra_4 = tokenizer.batch_encode_plus(train_4['overview'].values.tolist())\n",
    "train_encoded_electra_5 = tokenizer.batch_encode_plus(train_5['overview'].values.tolist())\n",
    "train_encoded_electra_6 = tokenizer.batch_encode_plus(train_6['overview'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7602fb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_length(cal_length):\n",
    "    num_tokens = [len(tokens) for tokens in cal_length]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "    \n",
    "    # 평균값, 최댓값, 표준편차\n",
    "    print(f\"토큰 길이 평균: {np.mean(num_tokens)}\")\n",
    "    print(f\"토큰 길이 최대: {np.max(num_tokens)}\")\n",
    "    print(f\"토큰 길이 표준편차: {np.std(num_tokens)}\")\n",
    "\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    maxlen = int(max_tokens)\n",
    "    print(f'설정 최대 길이: {maxlen}')\n",
    "    print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 설정값인 {maxlen}에 포함됩니다.')\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2041abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_length_1 = train_encoded_electra_1['input_ids']\n",
    "cal_length_2 = train_encoded_electra_2['input_ids']\n",
    "cal_length_3 = train_encoded_electra_3['input_ids']\n",
    "cal_length_4 = train_encoded_electra_4['input_ids']\n",
    "cal_length_5 = train_encoded_electra_5['input_ids']\n",
    "cal_length_6 = train_encoded_electra_6['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3dd2f724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 길이 평균: 253.10936591041303\n",
      "토큰 길이 최대: 1760\n",
      "토큰 길이 표준편차: 174.92996594018118\n",
      "설정 최대 길이: 602\n",
      "전체 문장의 0.951716114019779%가 설정값인 602에 포함됩니다.\n",
      "\n",
      "\n",
      "토큰 길이 평균: 169.89659134431253\n",
      "토큰 길이 최대: 995\n",
      "토큰 길이 표준편차: 88.54029689647899\n",
      "설정 최대 길이: 346\n",
      "전체 문장의 0.9747223286097281%가 설정값인 346에 포함됩니다.\n",
      "\n",
      "\n",
      "토큰 길이 평균: 101.56412866449512\n",
      "토큰 길이 최대: 599\n",
      "토큰 길이 표준편차: 65.08838234415482\n",
      "설정 최대 길이: 231\n",
      "전체 문장의 0.9704804560260586%가 설정값인 231에 포함됩니다.\n",
      "\n",
      "\n",
      "토큰 길이 평균: 239.2718204488778\n",
      "토큰 길이 최대: 4516\n",
      "토큰 길이 표준편차: 201.5714745492931\n",
      "설정 최대 길이: 642\n",
      "전체 문장의 0.9631278945493409%가 설정값인 642에 포함됩니다.\n",
      "\n",
      "\n",
      "토큰 길이 평균: 175.34449093444908\n",
      "토큰 길이 최대: 1157\n",
      "토큰 길이 표준편차: 139.02655652085429\n",
      "설정 최대 길이: 453\n",
      "전체 문장의 0.9665271966527197%가 설정값인 453에 포함됩니다.\n",
      "\n",
      "\n",
      "토큰 길이 평균: 152.3735632183908\n",
      "토큰 길이 최대: 648\n",
      "토큰 길이 표준편차: 88.72460680566985\n",
      "설정 최대 길이: 329\n",
      "전체 문장의 0.9454022988505747%가 설정값인 329에 포함됩니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token_length(cal_length_1)\n",
    "token_length(cal_length_2)\n",
    "token_length(cal_length_3)\n",
    "token_length(cal_length_4)\n",
    "token_length(cal_length_5)\n",
    "token_length(cal_length_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a556fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_length = 650"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da9be6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_overflowing_token(train_encoded_electra, train):\n",
    "    temp = []\n",
    "    for i in range(len(train_encoded_electra['input_ids'])):\n",
    "        if len(train_encoded_electra['input_ids'][i]) > select_length:\n",
    "            temp.append(i)\n",
    "\n",
    "    temp.reverse()\n",
    "\n",
    "    for i in temp:\n",
    "        train.drop(train.index[i], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fd3f4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719\n",
      "2611\n",
      "4912\n",
      "5614\n",
      "1434\n",
      "696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_1)}\\n{len(train_2)}\\n{len(train_3)}\\n{len(train_4)}\\n{len(train_5)}\\n{len(train_6)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "744b8722",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\frame.py:4906: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "drop_overflowing_token(train_encoded_electra_1, train_1)\n",
    "drop_overflowing_token(train_encoded_electra_2, train_2)\n",
    "drop_overflowing_token(train_encoded_electra_3, train_3)\n",
    "drop_overflowing_token(train_encoded_electra_4, train_4)\n",
    "drop_overflowing_token(train_encoded_electra_5, train_5)\n",
    "drop_overflowing_token(train_encoded_electra_6, train_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "894cc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1652\n",
      "2606\n",
      "4912\n",
      "5413\n",
      "1403\n",
      "696\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'{len(train_1)}\\n{len(train_2)}\\n{len(train_3)}\\n{len(train_4)}\\n{len(train_5)}\\n{len(train_6)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cd30d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_electra_1 = tokenizer.batch_encode_plus(train_1['overview'].values.tolist(), max_length=650, padding='max_length')\n",
    "train_encoded_electra_2 = tokenizer.batch_encode_plus(train_2['overview'].values.tolist(), max_length=650, padding='max_length')\n",
    "train_encoded_electra_3 = tokenizer.batch_encode_plus(train_3['overview'].values.tolist(), max_length=650, padding='max_length')\n",
    "train_encoded_electra_4 = tokenizer.batch_encode_plus(train_4['overview'].values.tolist(), max_length=650, padding='max_length')\n",
    "train_encoded_electra_5 = tokenizer.batch_encode_plus(train_5['overview'].values.tolist(), max_length=650, padding='max_length')\n",
    "train_encoded_electra_6 = tokenizer.batch_encode_plus(train_6['overview'].values.tolist(), max_length=650, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0ac71bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cal_length = train_encoded_electra_1['input_ids']\n",
    "num_tokens = [len(tokens) for tokens in cal_length]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "plt.title('all text length')\n",
    "plt.hist(num_tokens, bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad8d174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_1():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(20, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "778bdc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_1['input_ids'], train_1.cat3.values,\n",
    "                                                      test_size=0.2,stratify=train_1.cat3.values ,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53faf00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "050f2c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['electra.embeddings.position_ids', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x000001D98C110F70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x000001D98C110F70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = cat3_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cedfddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_1/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a78e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ffcd23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "826/826 [==============================] - 195s 226ms/step - loss: 2.5853 - accuracy: 0.1925 - val_loss: 2.0339 - val_accuracy: 0.3505\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/koelectra_cat3_detail_1\\\n",
      "Epoch 2/5\n",
      "826/826 [==============================] - 185s 224ms/step - loss: 1.3948 - accuracy: 0.6114 - val_loss: 1.0789 - val_accuracy: 0.7069\n",
      "\n",
      "Epoch 00002: saving model to checkpoints/koelectra_cat3_detail_1\\\n",
      "Epoch 3/5\n",
      "826/826 [==============================] - 184s 223ms/step - loss: 0.7325 - accuracy: 0.8015 - val_loss: 0.8985 - val_accuracy: 0.7583\n",
      "\n",
      "Epoch 00003: saving model to checkpoints/koelectra_cat3_detail_1\\\n",
      "Epoch 4/5\n",
      "826/826 [==============================] - 185s 223ms/step - loss: 0.4664 - accuracy: 0.8789 - val_loss: 0.9232 - val_accuracy: 0.7553\n",
      "\n",
      "Epoch 00004: saving model to checkpoints/koelectra_cat3_detail_1\\\n",
      "Epoch 5/5\n",
      "826/826 [==============================] - 185s 224ms/step - loss: 0.3527 - accuracy: 0.9044 - val_loss: 0.9555 - val_accuracy: 0.7764\n",
      "\n",
      "Epoch 00005: saving model to checkpoints/koelectra_cat3_detail_1\\\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset,steps_per_epoch=len(train_1) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c73149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_2():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(31, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1934661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cat3_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d0164",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_2['input_ids'], train_2.cat3.values,\n",
    "                                                      test_size=0.2, stratify=train_2.cat3.values ,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9db326",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3d6d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_2/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history_2 = model.fit(train_dataset,steps_per_epoch=len(train_2) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e98e45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_3():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(8, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30e27f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'electra.embeddings.position_ids', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x0000013C0C9EFEE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x0000013C0C9EFEE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = cat3_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ab5310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_3['input_ids'], train_3.cat3.values,\n",
    "                                                      test_size=0.2, stratify=train_3.cat3.values ,random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fba825a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04dacdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2456/2456 [==============================] - 558s 224ms/step - loss: 1.0110 - accuracy: 0.7107 - val_loss: 0.6354 - val_accuracy: 0.8464\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/koelectra_cat3_detail_3\\\n",
      "Epoch 2/5\n",
      "2456/2456 [==============================] - 547s 223ms/step - loss: 0.3789 - accuracy: 0.8907 - val_loss: 0.2426 - val_accuracy: 0.9430\n",
      "\n",
      "Epoch 00002: saving model to checkpoints/koelectra_cat3_detail_3\\\n",
      "Epoch 3/5\n",
      "2456/2456 [==============================] - 549s 223ms/step - loss: 0.2084 - accuracy: 0.9404 - val_loss: 0.2556 - val_accuracy: 0.9359\n",
      "\n",
      "Epoch 00003: saving model to checkpoints/koelectra_cat3_detail_3\\\n",
      "Epoch 4/5\n",
      "2456/2456 [==============================] - 548s 223ms/step - loss: 0.1364 - accuracy: 0.9632 - val_loss: 0.2402 - val_accuracy: 0.9410\n",
      "\n",
      "Epoch 00004: saving model to checkpoints/koelectra_cat3_detail_3\\\n",
      "Epoch 5/5\n",
      "2456/2456 [==============================] - 547s 223ms/step - loss: 0.0941 - accuracy: 0.9762 - val_loss: 0.2348 - val_accuracy: 0.9471\n",
      "\n",
      "Epoch 00005: saving model to checkpoints/koelectra_cat3_detail_3\\\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_3/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history_3 = model.fit(train_dataset,steps_per_epoch=len(train_3) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5afda656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_4():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(53, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d26e8f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x000001790CCE9EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x000001790CCE9EE0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = cat3_4()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_4['input_ids'], train_4.cat3.values,\n",
    "                                                      test_size=0.2, stratify=train_4.cat3.values ,random_state=777)\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2e1c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2706/2706 [==============================] - 609s 222ms/step - loss: 2.3686 - accuracy: 0.3915 - val_loss: 1.4486 - val_accuracy: 0.6260\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/koelectra_cat3_detail_4\\\n",
      "Epoch 2/5\n",
      "2706/2706 [==============================] - 600s 222ms/step - loss: 1.1455 - accuracy: 0.7053 - val_loss: 1.1179 - val_accuracy: 0.7091\n",
      "\n",
      "Epoch 00002: saving model to checkpoints/koelectra_cat3_detail_4\\\n",
      "Epoch 3/5\n",
      "2706/2706 [==============================] - 601s 222ms/step - loss: 0.7969 - accuracy: 0.7846 - val_loss: 1.0477 - val_accuracy: 0.7368\n",
      "\n",
      "Epoch 00003: saving model to checkpoints/koelectra_cat3_detail_4\\\n",
      "Epoch 4/5\n",
      "2706/2706 [==============================] - 600s 222ms/step - loss: 0.5620 - accuracy: 0.8400 - val_loss: 1.1597 - val_accuracy: 0.7470\n",
      "\n",
      "Epoch 00004: saving model to checkpoints/koelectra_cat3_detail_4\\\n",
      "Epoch 5/5\n",
      "2706/2706 [==============================] - 601s 222ms/step - loss: 0.4280 - accuracy: 0.8793 - val_loss: 1.1404 - val_accuracy: 0.7405\n",
      "\n",
      "Epoch 00005: saving model to checkpoints/koelectra_cat3_detail_4\\\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_4/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history_4 = model.fit(train_dataset,steps_per_epoch=len(train_4) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "002b6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_5():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(9, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b494926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function input_processing at 0x000002048C67BF70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function input_processing at 0x000002048C67BF70> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: '<' not supported between instances of 'str' and 'Literal'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5043: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = cat3_5()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_5['input_ids'], train_5.cat3.values,\n",
    "                                                      test_size=0.2, stratify=train_5.cat3.values ,random_state=777)\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dda76836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "701/701 [==============================] - 165s 224ms/step - loss: 1.4793 - accuracy: 0.4593 - val_loss: 1.1659 - val_accuracy: 0.6157\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/koelectra_cat3_detail_5\\\n",
      "Epoch 2/5\n",
      "701/701 [==============================] - 156s 222ms/step - loss: 0.8707 - accuracy: 0.7304 - val_loss: 0.8969 - val_accuracy: 0.7224\n",
      "\n",
      "Epoch 00002: saving model to checkpoints/koelectra_cat3_detail_5\\\n",
      "Epoch 3/5\n",
      "701/701 [==============================] - 156s 222ms/step - loss: 0.5918 - accuracy: 0.8074 - val_loss: 0.8579 - val_accuracy: 0.7509\n",
      "\n",
      "Epoch 00003: saving model to checkpoints/koelectra_cat3_detail_5\\\n",
      "Epoch 4/5\n",
      "701/701 [==============================] - 155s 222ms/step - loss: 0.3725 - accuracy: 0.8823 - val_loss: 0.7443 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00004: saving model to checkpoints/koelectra_cat3_detail_5\\\n",
      "Epoch 5/5\n",
      "701/701 [==============================] - 156s 222ms/step - loss: 0.2327 - accuracy: 0.9379 - val_loss: 0.7886 - val_accuracy: 0.7758\n",
      "\n",
      "Epoch 00005: saving model to checkpoints/koelectra_cat3_detail_5\\\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_5/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history_5 = model.fit(train_dataset,steps_per_epoch=len(train_5) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "278bc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_6():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(7, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eddc2427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = cat3_6()\n",
    "\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(train_encoded_electra_6['input_ids'], train_6.cat3.values,\n",
    "                                                      test_size=0.2, stratify=train_6.cat3.values ,random_state=777)\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices((x_train, y_train)).repeat().shuffle(2048).batch(2 * strategy.num_replicas_in_sync).prefetch(tf.data.experimental.AUTOTUNE))\n",
    "valid_dataset = (tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(2 * strategy.num_replicas_in_sync).cache().prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5253599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "348/348 [==============================] - 87s 226ms/step - loss: 1.5098 - accuracy: 0.3477 - val_loss: 1.4336 - val_accuracy: 0.3214\n",
      "\n",
      "Epoch 00001: saving model to checkpoints/koelectra_cat3_detail_6\\\n",
      "Epoch 2/5\n",
      "348/348 [==============================] - 77s 222ms/step - loss: 0.8345 - accuracy: 0.7328 - val_loss: 0.6686 - val_accuracy: 0.7429\n",
      "\n",
      "Epoch 00002: saving model to checkpoints/koelectra_cat3_detail_6\\\n",
      "Epoch 3/5\n",
      "348/348 [==============================] - 77s 222ms/step - loss: 0.5140 - accuracy: 0.8491 - val_loss: 0.8594 - val_accuracy: 0.7357\n",
      "\n",
      "Epoch 00003: saving model to checkpoints/koelectra_cat3_detail_6\\\n",
      "Epoch 4/5\n",
      "348/348 [==============================] - 77s 222ms/step - loss: 0.3242 - accuracy: 0.9023 - val_loss: 0.6937 - val_accuracy: 0.7643\n",
      "\n",
      "Epoch 00004: saving model to checkpoints/koelectra_cat3_detail_6\\\n",
      "Epoch 5/5\n",
      "348/348 [==============================] - 77s 222ms/step - loss: 0.2722 - accuracy: 0.9167 - val_loss: 0.8979 - val_accuracy: 0.7500\n",
      "\n",
      "Epoch 00005: saving model to checkpoints/koelectra_cat3_detail_6\\\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'checkpoints/koelectra_cat3_detail_6/'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history_6 = model.fit(train_dataset,steps_per_epoch=len(train_6) // 2 * strategy.num_replicas_in_sync,\n",
    "                    validation_data=valid_dataset,epochs= 5,\n",
    "                    callbacks=[cp_callback]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55950d5",
   "metadata": {},
   "source": [
    "## cat1 예측하는 KoELECTRA 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f16c2f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_KoELECTRA():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(500,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(6, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0acca5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2069a0633a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strategy = tf.distribute.get_strategy()\n",
    "saved_model = model_KoELECTRA()\n",
    "saved_model.load_weights('checkpoints/koelectra_cat1/koelectra_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4856670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "electra_tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48a8dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test['overview'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93291975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test_encoded_electra = electra_tokenizer.batch_encode_plus(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "16939604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰 길이 평균: 179.36771978021977\n",
      "토큰 길이 최대: 3744\n",
      "토큰 길이 표준편차: 153.0385736649946\n",
      "설정 최대 길이: 485\n",
      "전체 문장의 0.9608516483516484%가 설정값인 485에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "cal_length = test_encoded_electra['input_ids']\n",
    "num_tokens = [len(tokens) for tokens in cal_length]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 평균값, 최댓값, 표준편차\n",
    "print(f\"토큰 길이 평균: {np.mean(num_tokens)}\")\n",
    "print(f\"토큰 길이 최대: {np.max(num_tokens)}\")\n",
    "print(f\"토큰 길이 표준편차: {np.std(num_tokens)}\")\n",
    "\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print(f'설정 최대 길이: {maxlen}')\n",
    "print(f'전체 문장의 {np.sum(num_tokens < max_tokens) / len(num_tokens)}%가 설정값인 {maxlen}에 포함됩니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51860dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:2323: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_encoded_electra = tokenizer.batch_encode_plus(test_data, max_length=500, pad_to_max_length='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6af84003",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_encoded_electra['input_ids']\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(x_test).batch(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44e407e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7280"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e57eedc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7280/7280 [==============================] - 318s 43ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = saved_model.predict(test_dataset, verbose=1)\n",
    "pred_arg = pred.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6a94b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 2, 4], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cafda33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cat1_result'] = pred_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "249e04eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>overview</th>\n",
       "      <th>cat1_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>./image/test/TEST_00000.jpg</td>\n",
       "      <td>신선한 재료로 정성을 다해 만들었다. 늘 변함없는 맛과 서비스로 모실것을 약속한다.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>./image/test/TEST_00001.jpg</td>\n",
       "      <td>청청한 해역 등량만과 율포해수욕장이 한눈에 내려다 보이는 위치에 있으며, 막 잡은 ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>./image/test/TEST_00002.jpg</td>\n",
       "      <td>장터설렁탕은 남녀노소 누구나 즐길 수 있는 전통 건강식으로 좋은 재료와 전통 조리방...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>./image/test/TEST_00003.jpg</td>\n",
       "      <td>다양한 형태의 청소년수련활동을 제공함으로써 청소년들이 민주사회의 주역이 될 수 있도...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>./image/test/TEST_00004.jpg</td>\n",
       "      <td>팔공산은 경산시의 북쪽에 위치한 해발 1192.3 m의 높은 산으로 신라시대에는 중...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>TEST_07275</td>\n",
       "      <td>./image/test/TEST_07275.jpg</td>\n",
       "      <td>막국수와 수육을 주메뉴로 하며, 넓은 주차장이 마련되어 있어 주차하기 편리하다.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>TEST_07276</td>\n",
       "      <td>./image/test/TEST_07276.jpg</td>\n",
       "      <td>통진두레문화센터는 우리고유의 전통무형문화와 민속예술을 계승/발전 시키고, 다양한 문...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>TEST_07277</td>\n",
       "      <td>./image/test/TEST_07277.jpg</td>\n",
       "      <td>수도권에서 가까운 위치, 문산천을 따라 걷는 산책코스, 한여름 더위를 날려버릴 시원...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>TEST_07278</td>\n",
       "      <td>./image/test/TEST_07278.jpg</td>\n",
       "      <td>전남 구례군 관산리에 위치한 노고단 게스트하우스&amp;호텔은 지리산을 파노라마로 관망할 ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>TEST_07279</td>\n",
       "      <td>./image/test/TEST_07279.jpg</td>\n",
       "      <td>서귀포 산방산과 중문 관광단지 사이에 있는 건강과성박물관은 7000㎡ 크기의 실내 ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                     img_path  \\\n",
       "0     TEST_00000  ./image/test/TEST_00000.jpg   \n",
       "1     TEST_00001  ./image/test/TEST_00001.jpg   \n",
       "2     TEST_00002  ./image/test/TEST_00002.jpg   \n",
       "3     TEST_00003  ./image/test/TEST_00003.jpg   \n",
       "4     TEST_00004  ./image/test/TEST_00004.jpg   \n",
       "...          ...                          ...   \n",
       "7275  TEST_07275  ./image/test/TEST_07275.jpg   \n",
       "7276  TEST_07276  ./image/test/TEST_07276.jpg   \n",
       "7277  TEST_07277  ./image/test/TEST_07277.jpg   \n",
       "7278  TEST_07278  ./image/test/TEST_07278.jpg   \n",
       "7279  TEST_07279  ./image/test/TEST_07279.jpg   \n",
       "\n",
       "                                               overview  cat1_result  \n",
       "0        신선한 재료로 정성을 다해 만들었다. 늘 변함없는 맛과 서비스로 모실것을 약속한다.            3  \n",
       "1     청청한 해역 등량만과 율포해수욕장이 한눈에 내려다 보이는 위치에 있으며, 막 잡은 ...            3  \n",
       "2     장터설렁탕은 남녀노소 누구나 즐길 수 있는 전통 건강식으로 좋은 재료와 전통 조리방...            3  \n",
       "3     다양한 형태의 청소년수련활동을 제공함으로써 청소년들이 민주사회의 주역이 될 수 있도...            0  \n",
       "4     팔공산은 경산시의 북쪽에 위치한 해발 1192.3 m의 높은 산으로 신라시대에는 중...            5  \n",
       "...                                                 ...          ...  \n",
       "7275       막국수와 수육을 주메뉴로 하며, 넓은 주차장이 마련되어 있어 주차하기 편리하다.            3  \n",
       "7276  통진두레문화센터는 우리고유의 전통무형문화와 민속예술을 계승/발전 시키고, 다양한 문...            4  \n",
       "7277  수도권에서 가까운 위치, 문산천을 따라 걷는 산책코스, 한여름 더위를 날려버릴 시원...            0  \n",
       "7278  전남 구례군 관산리에 위치한 노고단 게스트하우스&호텔은 지리산을 파노라마로 관망할 ...            2  \n",
       "7279  서귀포 산방산과 중문 관광단지 사이에 있는 건강과성박물관은 7000㎡ 크기의 실내 ...            4  \n",
       "\n",
       "[7280 rows x 4 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4245f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(train['cat1'])\n",
    "train['cat1'] = encoder.transform(train['cat1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a18de8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['레포츠', '쇼핑', '숙박', '음식', '인문(문화/예술/역사)', '자연'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a421b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['cat1_result'] = encoder.inverse_transform(test['cat1_result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7029ecd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>overview</th>\n",
       "      <th>cat1_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>./image/test/TEST_00000.jpg</td>\n",
       "      <td>신선한 재료로 정성을 다해 만들었다. 늘 변함없는 맛과 서비스로 모실것을 약속한다.</td>\n",
       "      <td>음식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>./image/test/TEST_00001.jpg</td>\n",
       "      <td>청청한 해역 등량만과 율포해수욕장이 한눈에 내려다 보이는 위치에 있으며, 막 잡은 ...</td>\n",
       "      <td>음식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>./image/test/TEST_00002.jpg</td>\n",
       "      <td>장터설렁탕은 남녀노소 누구나 즐길 수 있는 전통 건강식으로 좋은 재료와 전통 조리방...</td>\n",
       "      <td>음식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>./image/test/TEST_00003.jpg</td>\n",
       "      <td>다양한 형태의 청소년수련활동을 제공함으로써 청소년들이 민주사회의 주역이 될 수 있도...</td>\n",
       "      <td>레포츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>./image/test/TEST_00004.jpg</td>\n",
       "      <td>팔공산은 경산시의 북쪽에 위치한 해발 1192.3 m의 높은 산으로 신라시대에는 중...</td>\n",
       "      <td>자연</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7275</th>\n",
       "      <td>TEST_07275</td>\n",
       "      <td>./image/test/TEST_07275.jpg</td>\n",
       "      <td>막국수와 수육을 주메뉴로 하며, 넓은 주차장이 마련되어 있어 주차하기 편리하다.</td>\n",
       "      <td>음식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7276</th>\n",
       "      <td>TEST_07276</td>\n",
       "      <td>./image/test/TEST_07276.jpg</td>\n",
       "      <td>통진두레문화센터는 우리고유의 전통무형문화와 민속예술을 계승/발전 시키고, 다양한 문...</td>\n",
       "      <td>인문(문화/예술/역사)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>TEST_07277</td>\n",
       "      <td>./image/test/TEST_07277.jpg</td>\n",
       "      <td>수도권에서 가까운 위치, 문산천을 따라 걷는 산책코스, 한여름 더위를 날려버릴 시원...</td>\n",
       "      <td>레포츠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7278</th>\n",
       "      <td>TEST_07278</td>\n",
       "      <td>./image/test/TEST_07278.jpg</td>\n",
       "      <td>전남 구례군 관산리에 위치한 노고단 게스트하우스&amp;호텔은 지리산을 파노라마로 관망할 ...</td>\n",
       "      <td>숙박</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7279</th>\n",
       "      <td>TEST_07279</td>\n",
       "      <td>./image/test/TEST_07279.jpg</td>\n",
       "      <td>서귀포 산방산과 중문 관광단지 사이에 있는 건강과성박물관은 7000㎡ 크기의 실내 ...</td>\n",
       "      <td>인문(문화/예술/역사)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7280 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                     img_path  \\\n",
       "0     TEST_00000  ./image/test/TEST_00000.jpg   \n",
       "1     TEST_00001  ./image/test/TEST_00001.jpg   \n",
       "2     TEST_00002  ./image/test/TEST_00002.jpg   \n",
       "3     TEST_00003  ./image/test/TEST_00003.jpg   \n",
       "4     TEST_00004  ./image/test/TEST_00004.jpg   \n",
       "...          ...                          ...   \n",
       "7275  TEST_07275  ./image/test/TEST_07275.jpg   \n",
       "7276  TEST_07276  ./image/test/TEST_07276.jpg   \n",
       "7277  TEST_07277  ./image/test/TEST_07277.jpg   \n",
       "7278  TEST_07278  ./image/test/TEST_07278.jpg   \n",
       "7279  TEST_07279  ./image/test/TEST_07279.jpg   \n",
       "\n",
       "                                               overview   cat1_result  \n",
       "0        신선한 재료로 정성을 다해 만들었다. 늘 변함없는 맛과 서비스로 모실것을 약속한다.            음식  \n",
       "1     청청한 해역 등량만과 율포해수욕장이 한눈에 내려다 보이는 위치에 있으며, 막 잡은 ...            음식  \n",
       "2     장터설렁탕은 남녀노소 누구나 즐길 수 있는 전통 건강식으로 좋은 재료와 전통 조리방...            음식  \n",
       "3     다양한 형태의 청소년수련활동을 제공함으로써 청소년들이 민주사회의 주역이 될 수 있도...           레포츠  \n",
       "4     팔공산은 경산시의 북쪽에 위치한 해발 1192.3 m의 높은 산으로 신라시대에는 중...            자연  \n",
       "...                                                 ...           ...  \n",
       "7275       막국수와 수육을 주메뉴로 하며, 넓은 주차장이 마련되어 있어 주차하기 편리하다.            음식  \n",
       "7276  통진두레문화센터는 우리고유의 전통무형문화와 민속예술을 계승/발전 시키고, 다양한 문...  인문(문화/예술/역사)  \n",
       "7277  수도권에서 가까운 위치, 문산천을 따라 걷는 산책코스, 한여름 더위를 날려버릴 시원...           레포츠  \n",
       "7278  전남 구례군 관산리에 위치한 노고단 게스트하우스&호텔은 지리산을 파노라마로 관망할 ...            숙박  \n",
       "7279  서귀포 산방산과 중문 관광단지 사이에 있는 건강과성박물관은 7000㎡ 크기의 실내 ...  인문(문화/예술/역사)  \n",
       "\n",
       "[7280 rows x 4 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4feffbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = test[test['cat1_result']=='자연']\n",
    "test_2 = test[test['cat1_result']=='레포츠']\n",
    "test_3 = test[test['cat1_result']=='음식'].\n",
    "test_4 = test[test['cat1_result']=='인문(문화/예술/역사)']\n",
    "test_5 = test[test['cat1_result']=='숙박']\n",
    "test_6 = test[test['cat1_result']=='쇼핑']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b77c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat3_1():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(20, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cat3_2():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(31, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cat3_3():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(8, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cat3_4():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(53, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cat3_5():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(9, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def cat3_6():\n",
    "    with strategy.scope():\n",
    "        encoder = TFElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", from_pt=True)\n",
    "\n",
    "        input_layer = Input(shape=(650,), dtype=tf.int32, name=\"input_layer\")\n",
    "        sequence_output = encoder(input_layer)[0]\n",
    "\n",
    "        cls_token = sequence_output[:, 0, :]\n",
    "\n",
    "        output_layer = Dense(7, activation='softmax')(cls_token)\n",
    "\n",
    "        model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        model.compile(Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c6a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n",
      "C:\\Users\\Home\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'electra.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFElectraModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFElectraModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[35000,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AssignVariableOp]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [63], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cat3_1 \u001b[38;5;241m=\u001b[39m cat3_1()\n\u001b[0;32m      2\u001b[0m cat3_2 \u001b[38;5;241m=\u001b[39m cat3_2()\n\u001b[1;32m----> 3\u001b[0m cat3_3 \u001b[38;5;241m=\u001b[39m cat3_3()\n\u001b[0;32m      4\u001b[0m cat3_4 \u001b[38;5;241m=\u001b[39m cat3_4()\n\u001b[0;32m      5\u001b[0m cat3_5 \u001b[38;5;241m=\u001b[39m cat3_5()\n",
      "Cell \u001b[1;32mIn [62], line 35\u001b[0m, in \u001b[0;36mcat3_3\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcat3_3\u001b[39m():\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m---> 35\u001b[0m         encoder \u001b[38;5;241m=\u001b[39m \u001b[43mTFElectraModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmonologg/koelectra-base-v3-discriminator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_pt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m         input_layer \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m650\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m         sequence_output \u001b[38;5;241m=\u001b[39m encoder(input_layer)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\modeling_tf_utils.py:2370\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2367\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[0;32m   2369\u001b[0m     \u001b[38;5;66;03m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[1;32m-> 2370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_checkpoint_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[0;32m   2372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2374\u001b[0m \u001b[38;5;66;03m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_weight_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:133\u001b[0m, in \u001b[0;36mload_pytorch_checkpoint_in_tf2_model\u001b[1;34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    129\u001b[0m     pt_state_dict\u001b[38;5;241m.\u001b[39mupdate(torch\u001b[38;5;241m.\u001b[39mload(pt_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    131\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch checkpoint contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(t\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m pt_state_dict\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_weights_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:258\u001b[0m, in \u001b[0;36mload_pytorch_weights_in_tf2_model\u001b[1;34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[0;32m    255\u001b[0m     weight_value_tuples\u001b[38;5;241m.\u001b[39mappend((symbolic_weight, array))\n\u001b[0;32m    256\u001b[0m     all_pytorch_weights\u001b[38;5;241m.\u001b[39mdiscard(name)\n\u001b[1;32m--> 258\u001b[0m \u001b[43mK\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_set_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_value_tuples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     tf_model(tf_inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Make sure restore ops are run\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:206\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m    209\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(wrapper, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\backend.py:3804\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[0;32m   3803\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:902\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    897\u001b[0m     tensor_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m    898\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    899\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot assign to variable\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m due to variable shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    900\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m are incompatible\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    901\u001b[0m       (tensor_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, value_tensor\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m--> 902\u001b[0m assign_op \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read_value:\n\u001b[0;32m    905\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_read(assign_op)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py:144\u001b[0m, in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 144\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m    146\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py:6897\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6895\u001b[0m message \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6896\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 6897\u001b[0m \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_status_to_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[35000,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:AssignVariableOp]"
     ]
    }
   ],
   "source": [
    "cat3_1 = cat3_1()\n",
    "cat3_2 = cat3_2()\n",
    "cat3_3 = cat3_3()\n",
    "cat3_4 = cat3_4()\n",
    "cat3_5 = cat3_5()\n",
    "cat3_6 = cat3_6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be854100",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"data/test_cat1_result.csv\", index=False, encoding='UTF8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1529de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tour",
   "language": "python",
   "name": "tour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
